# WildMatch: Reproducing the Zero-Shot Pipeline for Animal Species Recognition in Camera Trap Images

This repository aims to reproduce, as faithfully as possible, the **WildMatch** pipeline described in the paper:

> *“Multimodal Foundation Models for Zero-Shoot Animal Species Recognition in Camera Trap Images”*  :contentReference[oaicite:0]{index=0}  

WildMatch performs **zero-shot animal species classification** in camera trap images using **multimodal models (LLaVA-7B)** to generate detailed visual descriptions and a **LLM (GPT-4)** to match those descriptions with a **textual knowledge base** built from Wikipedia articles.

---

## 1. Overview of the WildMatch Method

The pipeline contains the following main components:

1. **Construction of a textual knowledge base (KB)**  
   - Species list extracted from camera trap datasets (LILA BC Camera Traps).  
   - Collection of Wikipedia articles for each species.  
   - *Visually Relevant Summaries* of the articles generated by GPT-4.

2. **Generation of visual descriptions for animal images**  
   - Use of a LMM (LLaVA-7B) to generate an initial caption.  
   - Post-processing of captions with:
     - **Color filtering** when color is not visible.  
     - **Expert knowledge augmentation** based on the species’ Wikipedia description.

3. **Instruction tuning of LLaVA-7B**  
   - Conversation-style dataset: image + instruction → detailed description.  
   - Instruction data:
     - Human-annotated dataset (**Manual-1.5k**).  
     - Large pseudo-labeled datasets (**LILA-10k** and **LILA-40k**).  
   - Training iterative generations of models (gen1 → gen2).

4. **Zero-shot classification via description matching**  
   - The LMM generates a detailed description.  
   - GPT-4 receives:
     - The KB (descriptions per class).  
     - The generated description.  
   - GPT-4 returns the most likely species (or genus/family/etc.).  
   - **Self-consistency**: sample N captions, then do majority vote.

5. **Hierarchical prediction**  
   - Predict at increasingly fine taxonomic levels.  
   - At each step, restrict KB to relevant subsets to fit LLM context window.

6. **Confidence estimation & calibration**  
   - Confidence comes from self-consistency frequencies.  
   - Used for calibration and for **human-in-the-loop pipelines**.

7. **Sequence-level prediction**  
   - Aggregate predictions from multiple frames belonging to the same trap sequence.  
   - Majority vote improves accuracy significantly.

---

## 2. Building the Textual Knowledge Base (KB)

### 2.1. Label source (species)

- Species are taken from **LILA BC Camera Traps**, covering **18 camera trap datasets**.
- For each species in those datasets (and those in the Magdalena evaluation dataset), the corresponding Wikipedia article is collected.

### 2.2. Extraction of relevant Wikipedia text

For each species:

1. Extract:
   - The **page summary**.
   - Sections with titles containing:
     - `description`, `characteristics`, `appearance`, `anatomy`.

2. Provide this text to GPT-4 to produce a **Visually Relevant Summary (VRS)**:
   - Only *visually observable* features are allowed.
   - No numerical measurements or non-visible traits.
   - Output must be a single paragraph.

**Prompt used (Appendix A):**

```text
System message: You are an AI assistant specialized in biology and providing accurate and
detailed descriptions of animal species.

Prompt: You are given the description of an animal species. Provide a very detailed
description of the appearance of the species and describe each body part of the animal
in detail. Only include details that can be directly visible in a photograph of the
animal. Only include information related to the appearance of the animal and nothing
else. Make sure to only include information that is present in the species description
and is certainly true for the given species. Do not include any information related
to the sound or smell of the animal. Do not include any numerical information related
to measurements in the text in units: m, cm, in, inches, ft, feet, km/h, kg, lb, lbs.
Remove any special characters such as unicode tags from the text. Return the answer as a
single paragraph. Species description: <WIKI_ARTICLE> Answer:
````

The output is a pair:

```text
<SPECIES>: <VISUALLY_RELEVANT_DESCRIPTION>
```

Stored in the KB.

---

## 3. Limitations of Generic LMMs (LLaVA, InstructBLIP, Otter, etc.)

The authors show that generic LMMs:

* **Fail to capture enough detail** for fine-grained species recognition.
* Produce **irrelevant scene commentary** (night, landscape, lighting).
* **Hallucinate**:

  * Incorrect colors (common in IR night images).
  * Nonexistent anatomical parts or extra animals.

Because of this, LLaVA-7B undergoes targeted **instruction tuning** to generate expert-level visual descriptions.

---

## 4. Generating Pseudo-Captions with Knowledge Augmentation

The objective is to convert the raw LLaVA caption into a **high-quality pseudo-caption** for use in training.

Two main steps:

1. **Color filtering**.
2. **Expert knowledge augmentation**.

### 4.1. Detecting low color variation in images

To determine whether color is discernible, use a central crop of the image:

For each pixel ( i ) with RGB channels ( R_i, G_i, B_i ):

[
\max_{i \in C} [ \max(|R_i - G_i|, |R_i - B_i|, |B_i - G_i|) ] < \epsilon
]

Where:

* ( C ): pixels in the central crop.
* ( \epsilon = 10 ).

If true → image is **monochromatic**, so color mentions are hallucinations.

### 4.2. Color filtering in pseudo-captions

For low-color-variation images, remove all color mentions except black/white.

**Prompt:**

```text
Prompt: This is the description of an animal in a photograph: <LMM_CAPTION>. Remove any
mentions of color other than black or white. Answer:
```

### 4.3. Expert knowledge augmentation

After color filtering, enrich the caption with expert species-level visual traits.

* Inputs:

  * **Expert description** from the KB.
  * **Filtered LMM caption**.

Goals:

1. Add only traits that could be visible in the photo.
2. Remove contradictions.
3. Do NOT mention the species name or try to guess it.

**Prompt (Appendix B):**

```text
System message: You are an AI assistant specialized in biology and providing accurate and
detailed descriptions of animal species.

Prompt: This is an expert description of the appearance of an animal species:
<EXPERT_DESCR>. This is an image description of the same species I can see in a
photograph: <LMM_CAPTION>. Imagine that you can also see this photo and perform the
following steps:
1. Rewrite the image description by adding details from the expert description of the
species that are visible in the photo. Make sure you only add details about body parts
of the animal already present in the image description.
2. Remove any information from the image description which directly contradicts the
expert description.
3. Do not mention the species name in the description and do not try to guess the
species.
Answer:
```

This output becomes the pseudo-caption for instruction tuning.

---

## 5. Instruction Tuning Dataset

Goal: build a dataset of conversations (instruction + image → detailed description).

### 5.1. Datasets

1. **LILA-10k**

   * Bounding box crops from LILA BC:

     * ~10k high-confidence detections from **MegaDetector**.
     * All human-annotated bboxes.
   * Max **25 crops per species**.
   * Pseudo-captions applied.

2. **LILA-40k**

   * Allows **100 crops per species** (~40k samples).
   * Same pseudo-caption process.

3. **Manual-1.5k (human-annotated dataset)**

   * ~1.5k images:

     * 2 highest-confidence crops per species (LILA BC).
     * * 1 clean Wikipedia image per species.
   * Process:

     * Extract visible traits from Wikipedia via GPT-4.
     * Human annotators label each trait as:

       * Fully visible / partially visible / not visible.
       * Whether colors can be discerned.
     * GPT-4 composes the final human-style caption including only visible traits.
     * If color is not visible, remove all color information.

This is used as a **gold standard**.

### 5.2. Instruction format

Each training sample is one-turn dialogue:

* Input: (image, instruction).
* Output: detailed caption.

Prompts are drawn from a list:

```text
- Give a very detailed visual description of the animal in the photo.
- Describe in detail the visible body parts of the animal in the photo.
- What are the visual characteristics of the animal in the photo?
- Describe the appearance of the animal in the photo.
- What are the identifying characteristics of the animal visible in the photo?
- How would you describe the animal in the photo?
- What does the animal in the photo look like?
```

Image can appear before or after the instruction.

---

## 6. Instruction Tuning LLaVA-7B

### 6.1. Base model

* Framework: **LLaVA**.
* Backbone: **LLaVA-7B**.

### 6.2. Model generations and training data

| Model           | LMM for pseudo-caption | Instruction data |
| --------------- | ---------------------- | ---------------- |
| LLaVA-7B-M-gen1 | –                      | Manual-1.5k      |
| LLaVA-7B-M-gen2 | LLaVA-7B-M-gen1        | LILA-40k         |
| LLaVA-7B-P-gen1 | LLaVA-7B               | LILA-10k         |
| LLaVA-7B-P-gen2 | LLaVA-7B-P-gen1        | LILA-40k         |

* **M** = models trained starting from human data.
* **P** = models trained using pseudo captions.
* **gen1/gen2** = training iteration.

### 6.3. Hardware

* Trained on **4 × A100 GPUs**.

---

## 7. Species Classification via Description Matching

### 7.1. Description generation

For each image:

1. Pass to the tuned LLaVA (e.g., **LLaVA-7B-M-gen2**).
2. Generate a detailed description.
3. Repeat N times (self-consistency).

### 7.2. Matching with the KB

GPT-4 receives:

* The **knowledge base**:

  ```text
  <SPECIES_1>: <DESCRIPTION_1>
  <SPECIES_2>: <DESCRIPTION_2>
  ...
  ```

* The generated LMM description.

* The list of labels: `<SPECIES_LIST>`.

**Prompt (Appendix G):**

```text
System message: You are an AI expert in biology specialized in animal species
identification.

Prompt: <KNOWLEDGE_BASE>
Question: You are given the following description of an animal: <LMM_CAPTION>. What
is the most likely animal being described from the following list: <SPECIES_LIST>. Make
sure your answer is a single word from the list <SPECIES_LIST>.
Answer:
```

Output is one species label.

### 7.3. Self-consistency & majority vote

For N descriptions, get N predictions:

[
{ y_1, y_2, \dots, y_N }
]

Final prediction:

[
\hat{y}(x) = \arg\max_{i \in Y} n_i(x)
]

Confidence:

[
c(x) = \frac{n_{\hat{y}(x)}(x)}{N}
]

---

## 8. Hierarchical Prediction (Taxonomy)

Taxonomic levels:

* Class → Order → Family → Genus → Species.

Process:

1. Predict at high level.
2. Filter KB to only descendants of that choice.
3. Repeat until level is fine-grained enough.

Useful to fit KB into GPT-4 context window.

---

## 9. Confidence Estimation from Self-Consistency

Approximate probability distribution:

[
p(i \mid x) \approx \frac{n_i(x)}{N}
]

Confidence:

[
c(x) = \frac{n_{\hat{y}(x)}(x)}{N}
]

Used for calibration and abstention.

---

## 10. Human-in-the-Loop Classification

Using a threshold ( p ):

* If ( c(x) \ge p ): accept.
* If ( c(x) < p ): abstain → human annotator.

Definitions:

[
A = { x_i \in D_{test} \mid c(x_i) \ge p }
]

[
A_{corr} = { x_i \in A \mid \hat{y}(x_i) = y_i }
]

Abstain Rate:

[
AR = \frac{|D_{test}| - |A|}{|D_{test}|}
]

Confident Accuracy:

[
CA = \frac{|A_{corr}|}{|A|}
]

---

## 11. Sequence-Level Predictions

Group frames into sequences:

* For each sequence with ( F ) frames:

  * Each frame gets N predictions.
  * Total predictions: ( F \times N ).
* Majority vote across the entire sequence.

Reported improvement:

* Frame-level accuracy: **70.12% → 77.54%**.
* Macro accuracy: **64.75% → 71.73%**.

---

## 12. Calibration Metrics

Metrics:

* **ECE**, **ACE**, **MCE**.

Definitions:

[
ECE = \sum_{i=1}^{n} \frac{|B_i|}{N} \big| acc(B_i) - conf(B_i) \big|
]

[
ACE = \sum_{i=1}^{n} \frac{1}{n} \big| acc(B_i) - conf(B_i) \big|
]

[
MCE = \max_{i \in {1 .. n}} \big| acc(B_i) - conf(B_i) \big|
]

Results:

### Supervised (ResNet-50)

* ECE ≈ 0.0316
* MCE ≈ 0.3920
* ACE ≈ 0.1490

### WildMatch (Zero-shot)

* ECE ≈ 0.0406
* MCE ≈ 0.0872
* ACE ≈ 0.0118

WildMatch has **better worst-case calibration (MCE/ACE)**.

---

## 13. Caption Quality Evaluation with GPT-4

Metrics:

* **Relevance score**
* **Hallucination score**

Using expert descriptions as ground truth.

### 13.1. Relevance score prompt

```text
Prompt: You are given two descriptions of an image: Description A and Description B.
Description A is the correct and accurate description of the image. Your job is to score
on a scale from 1 to 10 how well Description B describes the image. Follow these rules:
1. Only give the maximum score of 10, if Description B contains all the information in
Description A.
2. Only give the score of 1 if Description B contains no information that is given in
Description A.
3. Otherwise, assign scores from 2 to 9 to assess how much information from Description
A is mentioned in Description B (the higher score the more information from Description A
is present in Description B).
4. Disregard any information in Description B that is not mentioned in Description A in
your scoring.
5. Your answer is a single score from 1 to 10 without accompanying explanation of the
score.

Description A: <EXPERT_DESCR>
Description B: <LMM_CAPTION>
Your score:
```

### 13.2. Hallucination score prompt

```text
Prompt: You are given two descriptions of an image: Description A and Description B.
Description A is the correct and accurate description of the image. Definition of a
hallucination: a hallucination is a detail in Description B that is not mentioned in
Description A. Your job is to score on a scale from 1 to 10 how accurately Description
B describes the image, assigning higher score to descriptions with less hallucinations.
Follow these rules:
1. Only give the maximum score of 10, if Description B contains all information from
Description A and Description B does not contain any hallucinations.
2. Only give the score of 1 if Description B contains no information that is given in
Description A, but may contain any number of hallucinated details.
3. Otherwise, assign scores from 2 to 9 to assess how much hallucinated information is
present in Description B: the higher the score the less hallucinations are present in
Description B.
5. Your answer is a single score from 1 to 10 without accompanying explanation of the
score.

Description A: <EXPERT_DESCR>
Description B: <LMM_CAPTION>
Your score:
```

Used to evaluate:

* LLaVA without tuning.
* LLaVA tuned with pseudo-captions.
* LLaVA tuned with human captions.

---

## 14. Datasets & Experimental Protocol

### 14.1. Evaluation dataset: Magdalena Camera Traps

* Region: **Magdalena Medio**, Colombia.
* Total: **41,904 samples**:

  * 33,569 train
  * 8,335 validation
* Labels at **genus** level (36 labels).
* Resolution: **256 × 256**.
* Benchmark subset:

  * 20 classes with real-world imbalance.
  * Metrics: micro accuracy, macro accuracy.

Crucially, **Magdalena is never used during model training**.

### 14.2. Baselines

* **Supervised:** ResNet-50.

* **Zero-shot:** CLIP (ViT-L/14):

  * Text embedding = label name.
  * Or category description (CuPL style).

* **WildMatch:**

  * LLaVA-M/P-gen1/gen2.
  * GPT-4 matching + KB.
  * Self-consistency N=5.
