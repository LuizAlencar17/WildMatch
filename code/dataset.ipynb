{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "METADATA_SERENGETI_PATH = \"/data/luiz/dataset/serengeti/SnapshotSerengeti_S1-11_v2.1.json\"\n",
    "DATA_SERENGETI_PATH = \"/data/luiz/dataset/serengeti_images/\"\n",
    "\n",
    "# METADATA_SERENGETI_PATH = \"C:\\\\Users\\\\fabio\\\\Documents\\\\Workspace\\\\my-repos\\\\WildMatch\\\\data\\\\serengeti\\\\metadata.json\"\n",
    "# DATA_SERENGETI_PATH = \"C:\\\\Users\\\\fabio\\\\Documents\\\\Workspace\\\\my-repos\\\\WildMatch\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_file_path):\n",
    "    print(f\"Loading JSON file from {json_file_path}\")\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def merge_annotations_and_images(data, image_base_path):\n",
    "    \"\"\"\n",
    "    Merge annotations with images data and filter out images that don't exist.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing 'annotations' and 'images' lists\n",
    "        image_base_path: Base path where images are stored\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with merged data (only for existing images)\n",
    "    \"\"\"\n",
    "    # Create a mapping from image_id to image metadata\n",
    "    images_dict = {img['id']: img for img in data['images']}\n",
    "    \n",
    "    # Create a mapping from image_id to annotation\n",
    "    annotations_dict = {ann['image_id']: ann for ann in data['annotations']}\n",
    "    \n",
    "    merged_data = []\n",
    "    missing_images = []\n",
    "    \n",
    "    print(f\"Total images in metadata: {len(data['images'])}\")\n",
    "    print(f\"Total annotations: {len(data['annotations'])}\")\n",
    "    print(f\"\\nChecking which images exist on disk...\")\n",
    "    \n",
    "    for img in tqdm(data['images']):\n",
    "        image_id = img['id']\n",
    "        file_name = img['file_name']\n",
    "        \n",
    "        # Construct full image path\n",
    "        image_path = os.path.join(image_base_path, file_name)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if os.path.exists(image_path):\n",
    "            # Get corresponding annotation if it exists\n",
    "            annotation = annotations_dict.get(image_id, None)\n",
    "            \n",
    "            # Merge image and annotation data\n",
    "            merged_item = {\n",
    "                **img,  # Include all image metadata\n",
    "                'full_path': image_path,\n",
    "            }\n",
    "            \n",
    "            # Add annotation data if available\n",
    "            if annotation:\n",
    "                merged_item.update({\n",
    "                    'annotation_id': annotation['id'],\n",
    "                    'category_id': annotation['category_id'],\n",
    "                    'seq_id': annotation['seq_id'],\n",
    "                    'season': annotation['season'],\n",
    "                    'subject_id': annotation['subject_id'],\n",
    "                    'count': annotation['count'],\n",
    "                    'standing': annotation['standing'],\n",
    "                    'resting': annotation['resting'],\n",
    "                    'moving': annotation['moving'],\n",
    "                    'interacting': annotation['interacting'],\n",
    "                    'young_present': annotation['young_present'],\n",
    "                })\n",
    "            else:\n",
    "                merged_item['annotation_id'] = None\n",
    "                merged_item['category_id'] = None\n",
    "            \n",
    "            merged_data.append(merged_item)\n",
    "        else:\n",
    "            missing_images.append({\n",
    "                'image_id': image_id,\n",
    "                'file_name': file_name,\n",
    "                'expected_path': image_path\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(merged_data)} existing images\")\n",
    "    print(f\"✗ Missing {len(missing_images)} images\")\n",
    "    \n",
    "    return merged_data, missing_images\n",
    "\n",
    "# Function to balance the dataset\n",
    "def balance_dataset(df, category_col, min_samples=100):\n",
    "    # Count samples per class\n",
    "    class_counts = df[category_col].value_counts()\n",
    "    \n",
    "    # Target size = smallest class count\n",
    "    min_size = class_counts.min()\n",
    "    if min_size > min_samples:\n",
    "        min_size = min_samples\n",
    "\n",
    "    balanced_list = []\n",
    "\n",
    "    # Downsample each class to min_size\n",
    "    for class_value in class_counts.index:\n",
    "        df_class = df[df[category_col] == class_value]\n",
    "        df_downsampled = resample(\n",
    "            df_class,\n",
    "            replace=False,\n",
    "            n_samples=min_size,\n",
    "            random_state=123\n",
    "        )\n",
    "        balanced_list.append(df_downsampled)\n",
    "    \n",
    "    # Combine all classes\n",
    "    df_balanced = pd.concat(balanced_list).reset_index(drop=True)\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Function to copy images to a specified directory\n",
    "def copy_images_to_directory(df, source_col, dest_dir):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    \n",
    "    paths = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Copying images\"):\n",
    "        src = row[source_col]\n",
    "        paths.append(os.path.join(dest_dir, os.path.basename(src)))\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest_dir)\n",
    "\n",
    "    df[source_col] = paths\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_TO_REMOVE = [0, 1, 23]\n",
    "\n",
    "SPECIES_TO_INCLUDE = [\n",
    "  \"elephant\",\n",
    "  \"ostrich\",\n",
    "  \"zebra\",\n",
    "  \"cheetah\",\n",
    "  \"hippopotamus\",\n",
    "  \"baboon\",\n",
    "  \"buffalo\",\n",
    "  \"giraffe\",\n",
    "  \"warthog\",\n",
    "  \"guineafowl\",\n",
    "  \"hyenaspotted\",\n",
    "  \"impala\"\n",
    "]\n",
    "\n",
    "CSV_PATH = '../data/serengeti/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON file from /data/luiz/dataset/serengeti/SnapshotSerengeti_S1-11_v2.1.json\n",
      "Total images in metadata: 7178440\n",
      "Total annotations: 7261545\n",
      "\n",
      "Checking which images exist on disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7178440/7178440 [02:13<00:00, 53875.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 3197506 existing images\n",
      "✗ Missing 3980934 images\n"
     ]
    }
   ],
   "source": [
    "data = load_json(METADATA_SERENGETI_PATH)\n",
    "\n",
    "# Merge data and filter existing images\n",
    "merged_data, missing_images = merge_annotations_and_images(data, DATA_SERENGETI_PATH)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_original = pd.DataFrame(merged_data)\n",
    "\n",
    "# Filter out unwanted categories\n",
    "df = df_original[~df_original['category_id'].isin(CATEGORIES_TO_REMOVE)]\n",
    "\n",
    "# Create a mapping from category_id to species name\n",
    "category_map = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "\n",
    "# Add species_name column\n",
    "df['species_name'] = df['category_id'].map(category_map)\n",
    "\n",
    "print(\"Species distribution:\")\n",
    "print(df['species_name'].value_counts())\n",
    "print(f\"\\nTotal unique species: {df['species_name'].nunique()}\")\n",
    "print(f\"Samples with unknown category: {df['species_name'].isna().sum()}\")\n",
    "\n",
    "df = df[df['species_name'].isin(SPECIES_TO_INCLUDE)]\n",
    "df['species_name'].value_counts()\n",
    "\n",
    "# Balance the dataset\n",
    "df_balanced = balance_dataset(df, 'category_id')\n",
    "\n",
    "# Copy images to the specified directory\n",
    "df_balanced = copy_images_to_directory(df_balanced, 'full_path', '../data/serengeti/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images:   0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images: 100%|██████████| 1200/1200 [00:00<00:00, 3362.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the balanced dataset to CSV\n",
    "df_balanced.to_csv(CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
